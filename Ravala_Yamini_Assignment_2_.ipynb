{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaminiravala/5731/blob/main/Ravala_Yamini_Assignment_2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from [Semantic Scholar](https://www.semanticscholar.org).\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 reddits by using a hashtag (you can use any hashtag) from Reddits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E20pl1dR1Mym",
        "outputId": "4fac6a19-7818-416f-b274-905753485be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "srmtd_reviews.csv.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "# Here i selected a popular movie in Telugu language.\n",
        "url = \"https://www.imdb.com/title/tt4727512/reviews?ref_=tt_ql_3\"\n",
        "# Sending an HTTP GET request to the IMDb movie reviews page\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    # parsing the HTML content of the page\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    # extracting all the reviews\n",
        "    reviews = []\n",
        "    review_elements = soup.find_all(\"div\", class_=\"text show-more__control\")\n",
        "    for review_element in review_elements:\n",
        "        review_text = review_element.get_text()\n",
        "        reviews.append(review_text.strip())\n",
        "    #  i saved all the reviews to a CSV file\n",
        "    with open(\"srmtd_reviews.csv\", \"w\", newline=\"\") as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow([\"Review\"])\n",
        "        for review in reviews:\n",
        "            writer.writerow([review])\n",
        "    print(\"srmtd_reviews.csv.\")\n",
        "else:\n",
        "    print(\"Failed to retrieve the data.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "549f1285-f190-4b05-b66c-f2e8767321c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "srmtd_reviews_cleaned.csv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]   Package omw is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.data.find('corpora/omw.zip')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "df = pd.read_csv(\"srmtd_reviews.csv\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# function to clean and preprocess the text\n",
        "def clean_text(text):\n",
        "    # This will remove punctuatiuons\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # to remove numbers\n",
        "    text = ''.join([i for i in text if not i.isdigit()])\n",
        "    # here im atrying to removing the stopwords\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    cleaned_text = ' '.join(lemmatized_tokens)\n",
        "    return cleaned_text\n",
        "df['Cleaned_Review'] = df['Review'].apply(clean_text)\n",
        "# create new CSV file inorder to stotre cleaned dataset\n",
        "df.to_csv(\"srmtd_reviews_cleaned.csv\", index=False)\n",
        "print(\"srmtd_reviews_cleaned.csv.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIJgS8dV1Myo",
        "outputId": "ad9ec0a7-e59a-4325-d805-8b4f4a27e98b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtwBxaN01Myo",
        "outputId": "2bfa6fb5-9ad1-45f3-eeae-5f90cd0cb101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nltk                          3.7\n",
            "spacy                         3.7.2\n",
            "spacy-legacy                  3.0.12\n",
            "spacy-loggers                 1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep typing-extensions\n",
        "!pip list | grep nltk\n",
        "!pip list | grep spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "289f8a23-4cc9-4603-c843-119b211158a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tag Counts:\n",
            "Counter({'NN': 93, 'DT': 66, 'IN': 64, 'VBZ': 42, 'JJ': 40, 'PRP': 34, 'RB': 30, 'NNP': 29, 'NNS': 29, 'CC': 28, ',': 25, '.': 17, 'VBP': 17, 'VB': 17, 'PRP$': 13, 'VBN': 11, 'TO': 11, 'VBG': 10, 'VBD': 8, 'CD': 7, 'WP': 7, 'WRB': 4, 'RP': 4, 'WDT': 3, 'MD': 3, 'PDT': 3, 'POS': 2, 'RBS': 2, 'RBR': 2, 'JJR': 1, 'JJS': 1})\n",
            "Named Entities:\n",
            "Entity: three, Label: CARDINAL\n",
            "Entity: Mahesh Babu, Label: PERSON\n",
            "Entity: Harsha, Label: PERSON\n",
            "Entity: Mahesh, Label: GPE\n",
            "Entity: two, Label: CARDINAL\n",
            "Entity: two, Label: CARDINAL\n",
            "Entity: Srimanthudu, Label: PERSON\n",
            "Entity: Srimanthudu, Label: PERSON\n",
            "Entity: Srimanthudu, Label: PERSON\n",
            "Entity: Harsha, Label: PERSON\n",
            "Entity: Harsha, Label: PERSON\n",
            "Entity: Harsha, Label: PERSON\n",
            "Entity: Babu, Label: PERSON\n",
            "Entity: Shruti Hassan, Label: PERSON\n",
            "Entity: Jagapathi Babu, Label: PERSON\n",
            "Entity: Rajendra Parsad, Label: PERSON\n",
            "Entity: Villains Sampath Raj, Label: PERSON\n",
            "Entity: Mukesh Rishi, Label: PERSON\n",
            "Entity: Vennela Kishore, Label: PERSON\n",
            "Entity: Ali, Label: PERSON\n",
            "Entity: almost 3 hours, Label: TIME\n",
            "Entity: Overall Srimanthudu, Label: PERSON\n",
            "Entity: Mahesh Babu, Label: PERSON\n",
            "Entity: Aagdu, Label: GPE\n",
            "Entity: 1, Label: CARDINAL\n",
            "Entity: 7/10, Label: CARDINAL\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
        "from collections import Counter\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# i took a sample sentence from one of the review\n",
        "sample_sentence = \"Srimanthudu works because of three reasons them being Mahesh Babu being used to his maxim potential, the role of Harsha fits him like a glove. The director made sure he used all of Mahesh's characteristics which have made him famous till now they are smartly placed into his character too look genuine.The other two reasons are the sensible story which connects with everyone from the family audiences to the masses and finally it was made sure the masses where fully pleased with some good action sequences, with some a whistle worth song or two too.Therefore a film like Srimanthudu is set for blockbuster status. But for me even though i read about the immense positive reviews about the film,I still found the story a little common and predictable. But what won me over was the hero's character and his characterization which I could relate too. And the films final message of how money is not the most important thing in life, how helping others grow with you is more important, it gives a real reason to live, and makes society move forward positively . We in the rustle, bustle of life forget about our roots.How people are struggling back home, it is our duty/debt to go back and make a change in any small way so we can see the growth of rural areas into a place where people want to live. In short after becoming successful in life don't forget the were you came from, and the people responsible for your success. A strong message like that is what makes Srimanthudu a winner.Story wise Srimanthudu tells the story of a man named Harsha, who is a son of a billionaire. His dad wants him to take over the business, but Harsha does not want too. Harsha is lost in his own world, he does not know what he really wants to do but he knows that he doesn't care about making loads of money, and that he loves helping people who are less privileged. He then eventually finds out he is interested in rural studies and pursues those studies, and doing so also falls in love with a fellow student. Through all this,conflicts keep on arising, and he finds himself heading to his native village. What is reason for him to go back to his native village, and is he successful in his reason is what makes up the the rest of the plot.To know more rush to the theatres.Acting wise its a Mahesh Babu show all the way, he rules the roost. He has been given an amazing character, and with it gives one of his career best performances. The superstar shines Shruti Hassan excels in a substantial role, she has has matured as an actor, and she looks the most beautiful she has ever on screen with this film.Jagapathi Babu and Rajendra Parsad impress. Villains Sampath Raj and Mukesh Rishi are the usual clichéd villains. Vennela Kishore and Ali provide ample laughs.The only minuses in the film are the excessive length of almost 3 hours plus and the slow pace, at times the film story doesn't move.Overall Srimanthudu all withstanding is a blockbuster all the way. The way the film has class and mass sensibilities mixed together, with a sensible story, the audiences will come in hordes.The film has Mahesh Babu like never before, and after the disappointing failures of Aagdu and 1 this provide a comeback of sorts.3.5/5* or 7/10\"\n",
        "#POS count\n",
        "def pos_counts(text):\n",
        "    tags = pos_tag(word_tokenize(text))\n",
        "    tag_counts = Counter(tag for word, tag in tags)\n",
        "    return tag_counts\n",
        "# Named Entity Recognition\n",
        "def named_entity_recognition(text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "# analyze the pos tags\n",
        "pos_tag_counts = pos_counts(sample_sentence)\n",
        "print(\"POS Tag Counts:\")\n",
        "print(pos_tag_counts)\n",
        "# analyze NER\n",
        "named_entities = named_entity_recognition(sample_sentence)\n",
        "print(\"Named Entities:\")\n",
        "for entity, label in named_entities:\n",
        "    print(f\"Entity: {entity}, Label: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general we can understand that constituency parsing tree provides a hierarchical view of the sentence's structure into phrases, whereas dependency parsing tree focuses on the grammatical relationships between words, showing how each word connects to others in the sentence."
      ],
      "metadata": {
        "id": "Ep578Y_ZHJ__"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}