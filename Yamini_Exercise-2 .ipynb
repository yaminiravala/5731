{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fW5_oFVd9-pY"
   },
   "source": [
    "## The second In-class-exercise (09/13/2023, 40 points in total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAzh1U0sE5I5"
   },
   "source": [
    "Kindly use the provided .ipynb document to write your code or respond to the questions. Avoid generating a new file.\n",
    "Execute all the cells before your final submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpgvZQdRE-HV"
   },
   "source": [
    "This in-class exercise is due tomorrow September 14, 2023 at 11:59 PM. No late submissions will be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QBZI-je9-pZ"
   },
   "source": [
    "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWoKpYQT9-pa"
   },
   "source": [
    "Question 1 (10 points): Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LmNR3kw9-pa"
   },
   "source": [
    "\n",
    "Research Question: \"How does the adoption of renewable energy sources impact the energy efficiency and environmental sustainability of urban areas, and what are the economic implications of such a transition?\"\n",
    "\n",
    "Data Collection:\n",
    "\n",
    "To answer this research question, a multifaceted approach to data collection is required, encompassing various types of data from different sources:\n",
    "\n",
    "1. **Energy Consumption Data :**\n",
    "   - Collect historical energy consumption data for urban areas of interest.\n",
    "   - Data sources can include utility companies, government records, and smart meters.\n",
    "   - Data should cover a multi-year period to capture trends.\n",
    "\n",
    "2. **Renewable Energy Adoption Data :**\n",
    "   - Gather data on the installation and utilization of renewable energy sources, such as solar panels and wind turbines.\n",
    "   - This data can come from government records, energy companies, and renewable energy organizations.\n",
    "\n",
    "3. **Environmental Data :**\n",
    "   - Collect data on air quality, greenhouse gas emissions, and other environmental indicators.\n",
    "   - Utilize government environmental agencies' data and conduct field measurements.\n",
    "   - Qualitative data can be gathered through surveys and interviews regarding environmental perceptions.\n",
    "\n",
    "4. **Economic Data :**\n",
    "   - Retrieve economic data related to the costs and benefits of renewable energy adoption.\n",
    "   - This can include data on subsidies, tax incentives, and economic growth.\n",
    "   - Economic modeling may also be needed to estimate long-term economic impacts.\n",
    "\n",
    "\n",
    "Data Quantity:\n",
    "\n",
    "The quantity of data required depends on the scope of the study and the level of detail needed for analysis. For a comprehensive analysis, several years of historical data for energy consumption and environmental indicators are typically necessary. Additionally, data should cover a diverse set of urban areas to ensure representativeness. It is recommended to collect data from at least 100 urban areas for robust statistical analysis.\n",
    "\n",
    "Data Collection and Storage Steps:\n",
    "\n",
    "Here's a high-level overview of the steps for collecting and saving the data:\n",
    "\n",
    "1. **Energy and Environmental Data:**\n",
    "   - Identify the sources of energy and environmental data.\n",
    "   - Develop a data collection plan to ensure regular updates.\n",
    "   - Store this data in a structured format, such as a database or CSV files.\n",
    "\n",
    "2. **Renewable Energy Adoption Data:**\n",
    "   - Obtain data from government agencies, energy companies, and research institutions.\n",
    "   - Organize the data by location and time.\n",
    "   - Store this data alongside energy consumption data for analysis.\n",
    "\n",
    "3. **Economic Data:**\n",
    "   - Retrieve economic data from government reports and databases.\n",
    "   - Ensure data is available for the same urban areas and time periods as other datasets.\n",
    "   - Store economic data in a separate database or spreadsheet.\n",
    "\n",
    "4. **Public Data:**\n",
    "   - Design surveys and interview questionnaires.\n",
    "   - Collect responses from residents in the selected urban areas.\n",
    "   - Combine qualitative responses with quantitative sentiment analysis data.\n",
    "   - Store survey data in a secure database.\n",
    "\n",
    "5. **Data Security:**\n",
    "   - Implement data security measures to protect sensitive information, including encryption and access controls.\n",
    "\n",
    "Once all data is collected and stored securely, you can proceed with data analysis techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlxTLRNm9-pa"
   },
   "source": [
    "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (2.27.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (4.11.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples saved to data_samples.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Here i took EIA webpage\n",
    "url = 'https://www.eia.gov/consumption/data.php'\n",
    "# I Sent an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Find all paragraphs on the page\n",
    "    paragraphs = soup.find_all('p')\n",
    "    # Collect 1000 data samples\n",
    "    data_samples = []\n",
    "    for paragraph in paragraphs:\n",
    "        # Extract the text from the paragraph\n",
    "        data = paragraph.get_text().strip()\n",
    "        # Add the data to the list of samples\n",
    "        data_samples.append(data)\n",
    "        # Check if we have collected 1000 samples, and break the loop if so\n",
    "        if len(data_samples) >= 1000:\n",
    "            break\n",
    "    # Saving data samples to a CSV file\n",
    "    with open('data_samples.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Data Sample'])\n",
    "        for sample in data_samples:\n",
    "            writer.writerow([sample])\n",
    "\n",
    "    print('Data samples saved to data_samples.csv')\n",
    "\n",
    "else:\n",
    "    print(f'Failed to retrieve data. Status code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "px6wgvog9-pa"
   },
   "source": [
    "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2013-2023).\n",
    "\n",
    "The following information of the article needs to be collected:\n",
    "\n",
    "(1) Title\n",
    "\n",
    "(2) Venue/journal/conference being published\n",
    "\n",
    "(3) Year\n",
    "\n",
    "(4) Authors\n",
    "\n",
    "(5) Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "P5rjlclf9-pb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 articles saved to scholar_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Variables initializations\n",
    "articles = []\n",
    "keyword = \"information retrieval\"\n",
    "years_to_check = 10\n",
    "current_year = 2023\n",
    "\n",
    "# Loop through the years\n",
    "for year in range(current_year, current_year - years_to_check, -1):\n",
    "    # Construct the URL for Google Scholar with the specified year and keyword\n",
    "    url = f\"https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q={keyword}&as_ylo={year}&as_yhi={year}\"\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Find all search result elements\n",
    "        results = soup.find_all('div', class_='gs_ri')\n",
    "        # Extract information from each search result\n",
    "        for result in results:\n",
    "            title = result.find('h3', class_='gs_rt').a.text.strip()\n",
    "            authors = result.find('div', class_='gs_a').text.strip()\n",
    "            venue_year = result.find('div', class_='gs_a').text.strip().split('-')[-1].strip()            \n",
    "            # Check if there's an abstract element\n",
    "            abstract_element = result.find('div', class_='gs_rs')\n",
    "            abstract = abstract_element.text.strip() if abstract_element else ''\n",
    "            # Append the article data to the list\n",
    "            articles.append({\n",
    "                'Title': title,\n",
    "                'Authors': authors,\n",
    "                'Venue/Year': venue_year,\n",
    "                'Abstract': abstract\n",
    "            })\n",
    "\n",
    "# Saved articles to a CSV file\n",
    "csv_filename = 'scholar_articles.csv'\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Title', 'Authors', 'Venue/Year', 'Abstract']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()  # Write the CSV header row    \n",
    "    for article in articles:\n",
    "        writer.writerow(article)\n",
    "print(f'{len(articles)} articles saved to {csv_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCQpbJnwTxAB"
   },
   "source": [
    "Do either of the question-4 tasks given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GT3CNj_V9-pb"
   },
   "source": [
    "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
    "\n",
    "The following information needs to be collected:\n",
    "\n",
    "(1) User_name\n",
    "\n",
    "(2) Posted time\n",
    "\n",
    "(3) Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FymVNKVi9-pb"
   },
   "outputs": [],
   "source": [
    "# You code here (Please add comments in the code):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOeAr9TJTBgS"
   },
   "source": [
    "Question 4 (10 points):\n",
    "\n",
    "In this task, you are required to identify and utilize online tools for web scraping data from websites without the need for coding, with a specific focus on Parsehub. The objective is to gather data and save it in formats like CSV, Excel, or any other suitable file format.\n",
    "\n",
    "You have to mention an introduction to the tool which ever you prefer to use, steps to follow for web scrapping and the final output of the data collected.\n",
    "\n",
    "Upload a document (Word or PDF File) in the same repository and you can add the link in the ipynb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N20TjXLmTG1u"
   },
   "outputs": [],
   "source": [
    "# https://github.com/yaminiravala/5731/blob/main/Exercise-2%20Q-4.docx"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
