{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaminiravala/5731/blob/main/Yamini_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "## The third In-class-exercise (due on 11:59 PM 10/08/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2htC-oV70ne"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "Sentiment Analysis of Product Reviews\n",
        "\n",
        "In this text classification task, i aim to build a machine learning model to analyze the sentiment of product reviews, determining whether a review is positive, negative, or neutral. This task is valuable for businesses to understand customer opinions and feedback on their products and make data-driven decisions for improvements.\n",
        "\n",
        "Some features for the Machine Learning Model:\n",
        "\n",
        "Bag of Words (BoW) / Term Frequency-Inverse Document Frequency (TF-IDF): BoW or TF-IDF representations capture the frequency of individual words in the text. They help the model understand which words are prominent in positive and negative reviews.\n",
        "\n",
        "Word Embeddings : It transform words into dense vectors, capturing semantic relationships between words. This helps the model understand context and meaning in reviews.\n",
        "\n",
        "N-grams: This represent sequences of 'n' adjacent words. They capture the ordering of words and phrases in text, allowing the model to identify more complex patterns in sentiment.\n",
        "\n",
        "Sentiment Lexicons : This assign sentiment scores to words. By incorporating these scores, the model can identify sentiment-bearing words and their polarities.\n",
        "\n",
        "Part-of-Speech Tags: POS tagging labels words in a sentence as nouns, verbs, adjectives, etc. Analyzing the distribution of POS tags can reveal syntactic patterns associated with sentiment."
      ],
      "metadata": {
        "id": "4S_OT7UVSy6E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_wItlXVDVYJl",
        "outputId": "e180cd52-98b3-4ade-ab63-64d672ab1dae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ad996d02-b30c-4919-bd30-dcacff711266"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Feature Matrix:\n",
            "[[0 1 1 1 1 0 0 0 1 0 0 0 1]\n",
            " [0 0 1 0 0 0 1 0 1 1 1 1 1]\n",
            " [1 0 0 1 0 1 0 1 1 0 0 0 1]]\n",
            "\n",
            "Feature Names :\n",
            "['about' 'amazing' 'is' 'it' 'love' 'neutral' 'of' 'okay' 'product'\n",
            " 'quality' 'terrible' 'the' 'this']\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Sample text\n",
        "sample_text = [\n",
        "    \"This product is amazing! I love it!\",\n",
        "    \"The quality of this product is terrible.\",\n",
        "    \"I'm neutral about this product. It's okay.\",\n",
        "]\n",
        "\n",
        "# creating a CountVectorizer instance\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# fit the vectorizer to the text and transform the text into a BoW representation\n",
        "bow_matrix = vectorizer.fit_transform(sample_text)\n",
        "# get the feature names in the vocabulary\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "# convert the BoW matrix to a dense array and display the result\n",
        "bow_array = bow_matrix.toarray()\n",
        "# print the BoW feature matrix\n",
        "print(\"Bag of Words Feature Matrix:\")\n",
        "print(bow_array)\n",
        "\n",
        "# print the feature names\n",
        "print(\"\\nFeature Names :\")\n",
        "print(feature_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "563f0da3-defa-4bf3-e731-aa863fc92cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "product: 0.8436\n",
            "this: 0.8436\n",
            "it: 0.7552\n",
            "is: 0.7185\n",
            "amazing: 0.5094\n",
            "love: 0.5094\n",
            "about: 0.4836\n",
            "neutral: 0.4836\n",
            "okay: 0.4836\n",
            "of: 0.4354\n",
            "quality: 0.4354\n",
            "terrible: 0.4354\n",
            "the: 0.4354\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "# here i'm using TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "# create a TfidfVectorizer instance\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# fit and transform the sample text into a TF-IDF representation\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(sample_text)\n",
        "\n",
        "# calculate the importance scores (sum of TF-IDF values for each feature)\n",
        "feature_importance_scores = np.sum(tfidf_matrix, axis=0)\n",
        "\n",
        "# get the feature names (words in the vocabulary)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# creating a dictionary to store feature names and their importance scores\n",
        "feature_importance_dict = {\n",
        "    feature_names[i]: feature_importance_scores[0, i]\n",
        "    for i in range(len(feature_names))\n",
        "}\n",
        "\n",
        "# sorting in descending order\n",
        "sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "# print the scores\n",
        "for feature, importance_score in sorted_features:\n",
        "    print(f\"{feature}: {importance_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5185db26-898a-42ff-8175-d0b004c48ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Documents:\n",
            "Rank 1: Similarity = 0.6566\n",
            "The quality of this product is terrible.\n",
            "\n",
            "Rank 2: Similarity = 0.6296\n",
            "I'm neutral about this product. It's okay.\n",
            "\n",
            "Rank 3: Similarity = 0.6286\n",
            "This product is amazing! I love it!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#sample query to test\n",
        "query = \"How is the product?\"\n",
        "# here i am loading pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "# tokenize and encode the query\n",
        "query_tokens = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "query_output = model(**query_tokens).last_hidden_state.mean(dim=1)\n",
        "#encode all documents\n",
        "document_tokens = tokenizer(sample_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "document_outputs = model(**document_tokens).last_hidden_state.mean(dim=1)\n",
        "# calculate cosine similarities\n",
        "similarities = cosine_similarity(query_output.detach().numpy(), document_outputs.detach().numpy())\n",
        "# flatten th array.\n",
        "similarities = similarities.flatten()\n",
        "\n",
        "# Rank indescending order\n",
        "sorted_indices = similarities.argsort()[::-1]\n",
        "sorted_documents = [sample_text[i] for i in sorted_indices]\n",
        "# print\n",
        "print(\"Ranked Documents:\")\n",
        "for i, doc in enumerate(sorted_documents, start=1):\n",
        "    print(f\"Rank {i}: Similarity = {similarities[sorted_indices[i-1]]:.4f}\")\n",
        "    print(doc)\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}