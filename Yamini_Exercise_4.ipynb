{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaminiravala/5731/blob/main/Yamini_Exercise_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-hoq-Hlz8di"
      },
      "source": [
        "# **The fourth in-class-exercise (40 points in total, 03/28/2022)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuEgxyM3z8dk"
      },
      "source": [
        "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcbXrVwnz8dk"
      },
      "source": [
        "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2UTgR6az8dl",
        "outputId": "bcb12186-ecda-4f97-da54-d888dd993fea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (4.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.26.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: pyLDAvis in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (3.4.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (1.0.2)\n",
            "Requirement already satisfied: gensim in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (4.1.2)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (2.1.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (1.3.2)\n",
            "Requirement already satisfied: numexpr in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (1.26.1)\n",
            "Requirement already satisfied: funcy in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scipy in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (1.11.3)\n",
            "Requirement already satisfied: jinja2 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (61.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from pandas>=2.0.0->pyLDAvis) (2021.3)\n",
            "Requirement already satisfied: six>=1.5 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from gensim->pyLDAvis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Requirement already satisfied: packaging in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from numexpr->pyLDAvis) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from packaging->numexpr->pyLDAvis) (3.0.4)\n",
            "Requirement already satisfied: nltk in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: click in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2022.3.15)\n",
            "Requirement already satisfied: joblib in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.3.2)\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# installed required dependencies\n",
        "!pip install gensim\n",
        "!pip install pyLDAvis\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "a6u0ti75GxTA",
        "outputId": "411316f7-dc13-4c46-c9fd-0f95414100a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/saicharanreddypotluri/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/saicharanreddypotluri/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# used the previous text corpus\n",
        "text_data = [\"This product is amazing! I love it!\",\n",
        "    \"The quality of this product is terrible.\",\n",
        "    \"I'm neutral about this product. It's okay.\"]\n",
        "# data preprocessing\n",
        "tokenized_data = [word_tokenize(text.lower()) for text in text_data]\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_data = [[word for word in tokens if word not in stop_words] for tokens in tokenized_data]\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "data_lemmatized = [[lemmatizer.lemmatize(word) for word in tokens] for tokens in filtered_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "3ARCh7Ozz8dm",
        "outputId": "8f7fb053-a913-4a75-f01a-4c49af1efaed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/saicharanreddypotluri/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/saicharanreddypotluri/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# used the previous text corpus\n",
        "text_data = [\"This product is amazing! I love it!\",\n",
        "    \"The quality of this product is terrible.\",\n",
        "    \"I'm neutral about this product. It's okay.\"]\n",
        "# data preprocessing\n",
        "tokenized_data = [word_tokenize(text.lower()) for text in text_data]\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_data = [[word for word in tokens if word not in stop_words] for tokens in tokenized_data]\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "data_lemmatized = [[lemmatizer.lemmatize(word) for word in tokens] for tokens in filtered_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGpopOEaz8dm",
        "outputId": "b9a5bed2-6e55-431e-bd12-c1b29a45bc84"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#imported necessary packages and libraries.\n",
        "from gensim import corpora\n",
        "from gensim.models import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim\n",
        "# Using preprocced data for the model and coherence values.\n",
        "dictionary = corpora.Dictionary(data_lemmatized)\n",
        "corpus = [dictionary.doc2bow(text) for text in data_lemmatized]\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values\n",
        "\n",
        "# here we are calling the function to find the number of topics.\n",
        "limit = 50\n",
        "start = 2\n",
        "step = 6\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=data_lemmatized, start=start, limit=limit, step=step)\n",
        "# To visualize the coherence score we are representing it in a plot\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p07sP3MPz8dm",
        "outputId": "dd69056a-8f93-4294-82f2-7f34b3e60138"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0, '0.091*\"product\" + 0.091*\".\" + 0.091*\"!\" + 0.091*\"terrible\" + 0.091*\"love\" + 0.091*\"amazing\" + 0.091*\"quality\" + 0.091*\"\\'s\" + 0.091*\"okay\" + 0.091*\"neutral\"')\n",
            "(1, '0.091*\".\" + 0.091*\"!\" + 0.091*\"product\" + 0.091*\"terrible\" + 0.091*\"love\" + 0.091*\"amazing\" + 0.091*\"quality\" + 0.091*\"\\'s\" + 0.091*\"neutral\" + 0.091*\"okay\"')\n",
            "(2, '0.358*\"!\" + 0.185*\"amazing\" + 0.185*\"product\" + 0.185*\"love\" + 0.012*\".\" + 0.012*\"quality\" + 0.012*\"terrible\" + 0.012*\"\\'s\" + 0.012*\"\\'m\" + 0.012*\"okay\"')\n",
            "(3, '0.091*\"product\" + 0.091*\".\" + 0.091*\"!\" + 0.091*\"love\" + 0.091*\"quality\" + 0.091*\"terrible\" + 0.091*\"amazing\" + 0.091*\"okay\" + 0.091*\"\\'s\" + 0.091*\"neutral\"')\n",
            "(4, '0.091*\"product\" + 0.091*\".\" + 0.091*\"!\" + 0.091*\"terrible\" + 0.091*\"love\" + 0.091*\"quality\" + 0.091*\"amazing\" + 0.091*\"neutral\" + 0.091*\"\\'m\" + 0.091*\"\\'s\"')\n",
            "(5, '0.091*\".\" + 0.091*\"product\" + 0.091*\"!\" + 0.091*\"terrible\" + 0.091*\"love\" + 0.091*\"quality\" + 0.091*\"amazing\" + 0.091*\"okay\" + 0.091*\"neutral\" + 0.091*\"\\'s\"')\n",
            "(6, '0.224*\"quality\" + 0.224*\"product\" + 0.224*\".\" + 0.224*\"terrible\" + 0.015*\"!\" + 0.015*\"love\" + 0.015*\"amazing\" + 0.015*\"\\'s\" + 0.015*\"okay\" + 0.015*\"\\'m\"')\n",
            "(7, '0.091*\"product\" + 0.091*\".\" + 0.091*\"terrible\" + 0.091*\"!\" + 0.091*\"quality\" + 0.091*\"love\" + 0.091*\"amazing\" + 0.091*\"\\'s\" + 0.091*\"neutral\" + 0.091*\"\\'m\"')\n",
            "(8, '0.091*\"product\" + 0.091*\".\" + 0.091*\"terrible\" + 0.091*\"!\" + 0.091*\"quality\" + 0.091*\"love\" + 0.091*\"amazing\" + 0.091*\"\\'s\" + 0.091*\"neutral\" + 0.091*\"okay\"')\n",
            "(9, '0.091*\"product\" + 0.091*\".\" + 0.091*\"!\" + 0.091*\"terrible\" + 0.091*\"love\" + 0.091*\"amazing\" + 0.091*\"quality\" + 0.091*\"\\'s\" + 0.091*\"okay\" + 0.091*\"neutral\"')\n",
            "(10, '0.091*\"product\" + 0.091*\".\" + 0.091*\"!\" + 0.091*\"love\" + 0.091*\"terrible\" + 0.091*\"quality\" + 0.091*\"amazing\" + 0.091*\"\\'s\" + 0.091*\"okay\" + 0.091*\"\\'m\"')\n",
            "(11, '0.091*\".\" + 0.091*\"product\" + 0.091*\"!\" + 0.091*\"terrible\" + 0.091*\"love\" + 0.091*\"quality\" + 0.091*\"amazing\" + 0.091*\"neutral\" + 0.091*\"okay\" + 0.091*\"\\'m\"')\n",
            "(12, '0.266*\".\" + 0.138*\"\\'m\" + 0.138*\"product\" + 0.138*\"neutral\" + 0.138*\"\\'s\" + 0.138*\"okay\" + 0.009*\"!\" + 0.009*\"terrible\" + 0.009*\"love\" + 0.009*\"quality\"')\n",
            "(13, '0.091*\".\" + 0.091*\"product\" + 0.091*\"terrible\" + 0.091*\"!\" + 0.091*\"love\" + 0.091*\"quality\" + 0.091*\"amazing\" + 0.091*\"\\'s\" + 0.091*\"okay\" + 0.091*\"\\'m\"')\n"
          ]
        }
      ],
      "source": [
        "# Based on the coherence score choose coherence values\n",
        "optimal_model = model_list[coherence_values.index(max(coherence_values))]\n",
        "# print topic\n",
        "topics = optimal_model.print_topics(num_words=10)\n",
        "for topic in topics:\n",
        "    print(topic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGEneLYwz8dn"
      },
      "source": [
        "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieW_w0VMz8dn",
        "outputId": "d89ffcc0-a0b3-41ad-dc8b-ae784b874107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 1: amazing love neutral terrible quality okay\n",
            "Topic 2: terrible quality neutral okay love amazing\n",
            "Topic 3: okay neutral love amazing terrible quality\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "# sample text data\n",
        "text_data = [\"This product is amazing! I love it!\",\n",
        "    \"The quality of this product is terrible.\",\n",
        "    \"I'm neutral about this product. It's okay.\"]\n",
        "# TF-IDF vectorizer to fit and transform data\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=1000, stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(text_data)\n",
        "# LSA\n",
        "num_topics = 3  # since we have 3 topics.\n",
        "lsa = TruncatedSVD(n_components=num_topics, random_state=42)\n",
        "topic_matrix = lsa.fit_transform(tfidf_matrix)\n",
        "# we are trying to print the top terms\n",
        "terms = tfidf_vectorizer.get_feature_names_out()\n",
        "for i, topic in enumerate(lsa.components_):\n",
        "    top_terms = [terms[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
        "    print(f\"Topic {i+1}: {' '.join(top_terms)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZcNHALmz8dn"
      },
      "source": [
        "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRLriH8yE_Vx"
      },
      "source": [
        "I tried to import the lad2vec but i am unable to do so. I tried with chatGPT as well but still i couldn't able to complte. I will try to figure it out later for my knowledge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXZEoaz-z8dn",
        "outputId": "ba61f6cc-3341-4076-f796-7496690aa7a8"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mGetoptError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/fancy_getopt.py:233\u001b[0m, in \u001b[0;36mFancyGetopt.getopt\u001b[0;34m(self, args, object)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 233\u001b[0m     opts, args \u001b[38;5;241m=\u001b[39m \u001b[43mgetopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetopt\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshort_opts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong_opts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m getopt\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m msg:\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/getopt.py:95\u001b[0m, in \u001b[0;36mgetopt\u001b[0;34m(args, shortopts, longopts)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m         opts, args \u001b[38;5;241m=\u001b[39m \u001b[43mdo_shorts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshortopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opts, args\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/getopt.py:195\u001b[0m, in \u001b[0;36mdo_shorts\u001b[0;34m(opts, optstring, shortopts, args)\u001b[0m\n\u001b[1;32m    194\u001b[0m opt, optstring \u001b[38;5;241m=\u001b[39m optstring[\u001b[38;5;241m0\u001b[39m], optstring[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mshort_has_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshortopts\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m optstring \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/getopt.py:211\u001b[0m, in \u001b[0;36mshort_has_arg\u001b[0;34m(opt, shortopts)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m shortopts\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m GetoptError(_(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moption -\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m not recognized\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m%\u001b[39m opt, opt)\n",
            "\u001b[0;31mGetoptError\u001b[0m: option -f not recognized",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDistutilsArgError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/core.py:135\u001b[0m, in \u001b[0;36msetup\u001b[0;34m(**attrs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     ok \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_command_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DistutilsArgError \u001b[38;5;28;01mas\u001b[39;00m msg:\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/dist.py:476\u001b[0m, in \u001b[0;36mDistribution.parse_command_line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m parser\u001b[38;5;241m.\u001b[39mset_aliases({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlicence\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlicense\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m--> 476\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetopt\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m option_order \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mget_option_order()\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/fancy_getopt.py:235\u001b[0m, in \u001b[0;36mFancyGetopt.getopt\u001b[0;34m(self, args, object)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m getopt\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m msg:\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DistutilsArgError(msg)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt, val \u001b[38;5;129;01min\u001b[39;00m opts:\n",
            "\u001b[0;31mDistutilsArgError\u001b[0m: option -f not recognized",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m kw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     12\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlda2vec\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     13\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     packages\u001b[38;5;241m=\u001b[39mfind_packages(),\n\u001b[1;32m     19\u001b[0m     url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/setuptools/__init__.py:87\u001b[0m, in \u001b[0;36msetup\u001b[0;34m(**attrs)\u001b[0m\n\u001b[1;32m     86\u001b[0m _install_setup_requires(attrs)\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/core.py:137\u001b[0m, in \u001b[0;36msetup\u001b[0;34m(**attrs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DistutilsArgError \u001b[38;5;28;01mas\u001b[39;00m msg:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m(gen_usage(dist\u001b[38;5;241m.\u001b[39mscript_name) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124merror: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m msg)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG:\n",
            "\u001b[0;31mSystemExit\u001b[0m: usage: ipykernel_launcher.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n   or: ipykernel_launcher.py --help [cmd1 cmd2 ...]\n   or: ipykernel_launcher.py --help-commands\n   or: ipykernel_launcher.py cmd --help\n\nerror: option -f not recognized",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:1972\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   1969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[1;32m   1970\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1971\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 1972\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1973\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1974\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1975\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1976\u001b[0m         \u001b[38;5;66;03m# Exception classes can customise their traceback - we\u001b[39;00m\n\u001b[1;32m   1977\u001b[0m         \u001b[38;5;66;03m# use this in IPython.parallel for exceptions occurring\u001b[39;00m\n\u001b[1;32m   1978\u001b[0m         \u001b[38;5;66;03m# in the engines. This should return a list of strings.\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py:585\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \n\u001b[1;32m    580\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py:443\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    440\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    441\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    442\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 443\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py:1118\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m tb\n\u001b[0;32m-> 1118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py:1012\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1009\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py:865\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    858\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    862\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m    863\u001b[0m ):\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[1;32m    869\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py:799\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    797\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(etype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[1;32m    798\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 799\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    802\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    803\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py:854\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[0;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m    848\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    849\u001b[0m options \u001b[38;5;241m=\u001b[39m stack_data\u001b[38;5;241m.\u001b[39mOptions(\n\u001b[1;32m    850\u001b[0m     before\u001b[38;5;241m=\u001b[39mbefore,\n\u001b[1;32m    851\u001b[0m     after\u001b[38;5;241m=\u001b[39mafter,\n\u001b[1;32m    852\u001b[0m     pygments_formatter\u001b[38;5;241m=\u001b[39mformatter,\n\u001b[1;32m    853\u001b[0m )\n\u001b[0;32m--> 854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFrameInfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[tb_offset:]\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py:546\u001b[0m, in \u001b[0;36mFrameInfo.stack_data\u001b[0;34m(cls, frame_or_tb, options, collapse_repeated_frames)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack_data\u001b[39m(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    536\u001b[0m         collapse_repeated_frames: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    537\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrameInfo\u001b[39m\u001b[38;5;124m'\u001b[39m, RepeatedFrames]]:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    An iterator of FrameInfo and RepeatedFrames objects representing\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03m    a full traceback or stack. Similar consecutive frames are collapsed into RepeatedFrames\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    and optionally an Options object to configure.\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m     stack \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miter_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_or_tb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;66;03m# Reverse the stack from a frame so that it's in the same order\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;66;03m# as the order from a traceback, which is the order of a printed\u001b[39;00m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;66;03m# traceback when read top to bottom (most recent call last)\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_frame(frame_or_tb):\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py:98\u001b[0m, in \u001b[0;36miter_stack\u001b[0;34m(frame_or_tb)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m frame_or_tb:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m frame_or_tb\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_or_tb\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     99\u001b[0m         frame_or_tb \u001b[38;5;241m=\u001b[39m frame_or_tb\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py:91\u001b[0m, in \u001b[0;36mis_frame\u001b[0;34m(frame_or_tb)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_frame\u001b[39m(frame_or_tb: Union[FrameType, TracebackType]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[43massert_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mframe_or_tb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFrameType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTracebackType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(frame_or_tb, (types\u001b[38;5;241m.\u001b[39mFrameType,))\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py:172\u001b[0m, in \u001b[0;36massert_\u001b[0;34m(condition, error)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    171\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mAssertionError\u001b[39;00m(error)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from setuptools import setup\n",
        "from setuptools import find_packages\n",
        "import os\n",
        "with open('requirements.txt') as f:\n",
        "    install_requires = f.read().splitlines()\n",
        "\n",
        "# If building on RTD, don't install anything\n",
        "if os.environ.get('READTHEDOCS', None) == 'True':\n",
        "    install_requires = []\n",
        "\n",
        "kw = dict(\n",
        "    name='lda2vec',\n",
        "    version='0.1',\n",
        "    description='Tools for interpreting natural language',\n",
        "    author='Christopher E Moody',\n",
        "    author_email='chrisemoody@gmail.com',\n",
        "    install_requires=install_requires,\n",
        "    packages=find_packages(),\n",
        "    url='')\n",
        "\n",
        "setup(**kw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5qkAfciz8do"
      },
      "source": [
        "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpoAFmqIz8do",
        "outputId": "cab23382-856d-4e69-c397-5a534fecd380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bertopic\n",
            "  Using cached bertopic-0.15.0-py2.py3-none-any.whl (143 kB)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from bertopic) (2.1.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from bertopic) (1.26.1)\n",
            "Collecting hdbscan>=0.8.29\n",
            "  Using cached hdbscan-0.8.33-cp39-cp39-macosx_10_9_x86_64.whl\n",
            "Requirement already satisfied: plotly>=4.7.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from bertopic) (5.6.0)\n",
            "Collecting umap-learn>=0.5.0\n",
            "  Using cached umap_learn-0.5.4-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from bertopic) (1.0.2)\n",
            "Collecting sentence-transformers>=0.4.1\n",
            "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from bertopic) (4.64.0)\n",
            "Requirement already satisfied: joblib>=1.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
            "Requirement already satisfied: cython<3,>=0.27 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.29->bertopic) (0.29.28)\n",
            "Requirement already satisfied: scipy>=1.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.29->bertopic) (1.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2021.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
            "Requirement already satisfied: six in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from plotly>=4.7.0->bertopic) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.2.0)\n",
            "Requirement already satisfied: torchvision in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.14.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.17.3)\n",
            "Collecting sentencepiece\n",
            "  Using cached sentencepiece-0.1.99-cp39-cp39-macosx_10_9_x86_64.whl (1.2 MB)\n",
            "Requirement already satisfied: nltk in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (3.7)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.34.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (1.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
            "Requirement already satisfied: filelock in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0)\n",
            "Requirement already satisfied: fsspec in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2022.2.0)\n",
            "Requirement already satisfied: requests in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.0.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.3.15)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.14.1)\n",
            "Collecting tbb>=2019.0\n",
            "  Using cached tbb-2021.10.0-py2.py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.whl (640 kB)\n",
            "Requirement already satisfied: numba>=0.51.2 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from umap-learn>=0.5.0->bertopic) (0.58.1)\n",
            "Collecting pynndescent>=0.5\n",
            "  Using cached pynndescent-0.5.10-py3-none-any.whl\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.1)\n",
            "Requirement already satisfied: click in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.0.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.9)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.0.1)\n",
            "Installing collected packages: tbb, sentencepiece, pynndescent, umap-learn, sentence-transformers, hdbscan, bertopic\n",
            "  Attempting uninstall: tbb\n",
            "    Found existing installation: TBB 0.2\n",
            "\u001b[31mERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjEcEaKWLrrS",
        "outputId": "6070ed54-c54a-452d-ede8-e08175250d86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/saicharanreddypotluri/opt/anaconda3/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
            "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
            "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
            "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: TBB 0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pip\n",
        "pip.main([\"uninstall\", \"TBB\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxvYUz1v3QVP",
        "outputId": "a762359a-2cd9-4c99-9972-f02239c15e96"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1822588724.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Input \u001b[0;32mIn [3]\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install --upgrade TBB\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "from bertopic import BERTopic\n",
        "from summarizer import Summarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvZJCcWl3aO5"
      },
      "outputs": [],
      "source": [
        "text_data = [\"This product is amazing! I love it!\",\n",
        "    \"The quality of this product is terrible.\",\n",
        "    \"I'm neutral about this product. It's okay.\"]\n",
        "documents = text_data.data\n",
        "# initialize the model BERTopic\n",
        "model = BERTopic(language=\"english\")\n",
        "# Fit the model to topic\n",
        "topics, _ = model.fit_transform(documents)\n",
        "# print the topic\n",
        "for topic_id, topic_words in topics.items():\n",
        "    print(f\"Topic {topic_id}: {', '.join(topic_words)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyLXigabLrrS"
      },
      "source": [
        "I was good till here and then i got an issue with TBB. I couldn't able to proceed furthur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-v2cMBBz8do"
      },
      "source": [
        "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhrezDgWz8dp"
      },
      "source": [
        "# Write your answer here (no code needed for this question)\n",
        "### Since i couldn't able to reach till the results for all four models i can explain the differences in breif about alll the four models.\n",
        "## LDA: Tis will be a good choice when we have a large corpus data and want interpretable results. Or if we want to identify topics from past textual data.\n",
        "## LSA: This is a good when we have a moderate huge corpus data and want semantic relationship.\n",
        "## LDA2Vec: This is a good choice when you need a balance between LDA and LSA.\n",
        "## BERT: When we want to have deep leverage conceptual embeddings but not the specific number of topics in advance."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}